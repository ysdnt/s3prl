runner:
  total_steps: 323730
  gradient_clipping: 1.0e+3
  gradient_accumulate_steps: 5

  log_step: 5
  eval_step: 1295
  save_step: 1295
  max_keep: 10
  eval_dataloaders: [dev]
  
# 20 1295 
# 1 25898
# 2 12949

optimizer: 
  name: TorchOptim
  torch_optim_name: AdamW
  lr: 1.0e-4

# # comment the whole scheduler config block to disable learning rate scheduling
# scheduler:
#   name: linear_schedule_with_warmup
#   num_warmup_steps: 6000

downstream_expert: 
  datarc:
    # In first time, we filter out utterances which shorter than two sec. (change config just influenced by cache files)
    # If you change min_sec, you need to delete cache file you create and rerun the code, so that the effect will apply.
    vad_config:
      min_sec: 16000

    # Zalo
    # file_path: /content/gdrive/MyDrive/KLTN/dataset/zalo_dataset/dataset_fix
    # train_meta_data: ../../../dataset/zalo_dataset/dataset_fix/train_list_id.txt    
    
    # Vivos
    # file_path: /content/gdrive/MyDrive/KLTN/dataset/vivos
    # train_meta_data: /content/gdrive/MyDrive/KLTN/dataset/vivos/train/train_list_id.txt
    
    # mix
    file_path: /kaggle/input/
    train_meta_data: /kaggle/working/s3prl/s3prl/downstream/sv_voxceleb1/configs/speaker_id_mix.txt

    # swapped
    dev_meta_data: /kaggle/working/s3prl/s3prl/downstream/sv_voxceleb1/configs/veri_val.txt
    test_meta_data: /kaggle/working/s3prl/s3prl/downstream/sv_voxceleb1/configs/veri_val.txt

    # VoxCeleb1
    # file_path: /content/gdrive/MyDrive/KLTN/dataset/voxceleb1
    # train_meta_data: /content/gdrive/MyDrive/KLTN/dataset/voxceleb1/set_all.txt
    # train_meta_data: /content/gdrive/MyDrive/KLTN/dataset/voxceleb1/speaker_id.txt
    
    # file_path: /content/gdrive/MyDrive/KLTN/dataset/voxceleb1/VoxCeleb1_1
    # train_meta_data: /content/gdrive/MyDrive/KLTN/dataset/voxceleb1/set_1.txt
    
    # dev_meta_data: ../../../dataset/voxceleb1/veri_val.txt
    # test_meta_data: ../../../dataset/voxceleb1/veri_val.txt

    max_timestep: 128000
    train_batch_size: 20
    eval_batch_size: 1
    num_workers: 8 

  modelrc:
    module:
      XVector  # support to [ XVector, Identity ]
    input_dim: 512
    agg_module: SP # support for ASP / SP / AP / MP 
                   # (Attentive Statistic Pooling / Statistic Pooling / Attentive Pooling / Mean Pooling)
    utter_module:
      UtteranceExtractor # support to [UtteranceExtractor, UtteranceIdentity]
    
    module_config:
      # You can comment it if you do not use this. To demo the usage, we will show all case.
      XVector:
        agg_dim: 1500
        dropout_p: 0.0
        batch_norm: False
      
      Identity:
        no_args: True
        # do nothing    
    
    ObjectiveLoss: AMSoftmaxLoss # You can specify config to AMSoftmaxLoss ,AAMSoftmaxLoss or SoftmaxLoss
    
    LossConfig:
      # You can comment it if you do not use this. To demo the usage of SoftmaxLoss, we will show all case.
      SoftmaxLoss: 
        no_args: True
    
      # You can comment it if you do not use this. To demo the usage of AMSoftmaxLoss, we will show all case.
      AMSoftmaxLoss:
        s: 30.0
        m: 0.4
      # You can comment it if you do not use this. To demo the usage of AMSoftmaxLoss, we will show all case.
      AAMSoftmaxLoss:
        s: 15
        m: 0.3
        easy_margin: False
